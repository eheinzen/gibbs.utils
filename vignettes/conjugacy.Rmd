---
title: "Conjugate distributions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Conjugate distributions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Below we derive the posterior distributions for various scenarios, given their conjugate priors.

# Means

## Matrix Normal Regression: beta


$\DeclareMathOperator{\vect}{vec}$
$\DeclareMathOperator{\Tr}{Tr}$
$\newcommand\Norm{\mathcal{N}}$
$\newcommand\Wish{\mathcal{W}}$

In this scenario, we look for the posterior for the model

$$Y \sim \mathcal{MN}_{n,p}(X\beta, U, V)$$
with $X\beta$ an $n$ by $p$ matrix, U a $n$ by $n$ precision matrix,
and V a $p$ by $p$ precision matrix.


We rewrite the model as
$$\vect(Y) \sim \Norm_{np}(\vect(X\beta), V \otimes U)$$
with
$$\vect(\beta) \sim \Norm_(\mu_0, Q_0)$$

Let's consider the likelihood

$$
L(\beta\ \big|\ U, V, Y, X, \mu_0, Q_0) \propto \exp\left\{-\frac{1}{2}(\vect(Y) - \vect(X\beta))^\intercal (V \otimes U) (\vect(Y) - \vect(X\beta)) +
  -\frac{1}{2}(\vect(\beta) - \mu_0)^\intercal Q_0 (\vect(\beta) - \mu_0)\right\}
$$

Expanding the inner part (ignoring the $-1/2$):

$$
\vect(Y)^\intercal (V \otimes U) \vect(Y)\\
- 2\vect(Y)^\intercal(V \otimes U)\vect(X\beta)\\
+ \vect(X\beta)^\intercal (V \otimes U) \vect(X\beta)\\
+ \vect{\beta}^\intercal Q_0 \vect{\beta}\\
- 2\mu_o^\intercal Q_0 \beta\\
+ \mu_0^\intercal Q_0 \mu_0
$$

Note that $\vect(X\beta) = (I_p \otimes X)\vect(\beta)$. Then
$$
\vect(X\beta)^\intercal = \vect(\beta)^\intercal(I_p \otimes X)^\intercal = \vect(\beta)^\intercal (I_p \otimes X^\intercal)
$$

Dropping unused terms and regrouping:

$$
\vect(\beta)^\intercal ((I_p \otimes X^\intercal) (V \otimes U) (I_p \otimes X) + Q_0)\vect(\beta)\\
- 2(\vect(Y)^\intercal(V \otimes U)(I_p \otimes X) + \mu_o^\intercal Q_0)\vect(\beta)
$$

The first line gives the posterior precision:

$$
\begin{align}
\tilde{Q} &= (I_p \otimes X^\intercal) (V \otimes U) (I_p \otimes X) + Q_0\\
&=(I_p \otimes X^\intercal) ((VI_p) \otimes (UX)) + Q_0\\
&= (I_pVI_p) \otimes (X^\intercal U X) + Q_0\\
&= V \otimes (X^\intercal U X) + Q_0
\end{align}
$$

Of course, if $U$ is the identity, we can precompute $X^\intercal X$ for additional efficiency.

The inverse of $\tilde{Q}$ is used to complete the square, and together with the transpose of the second line gives us the posterior mean.
We gain additional efficiency with one more trick: for appropriate matrices, $(C^\intercal \otimes A) \vect(B) = \vect(ABC)$. Note that $V$ is symmetric.

$$
\begin{align}
\tilde{\mu} &= \tilde{Q}^{-1}((I_p \otimes X^\intercal) (V \otimes U) \vect{Y} + Q_0 \mu_0) \\
&= \tilde{Q}^{-1}\left[((I_pV) \otimes (X^\intercal U))\vect{Y} + Q_0 \mu_0 \right] \\
&= \tilde{Q}^{-1}\left[(V \otimes (X^\intercal U))\vect{Y} + Q_0 \mu_0 \right] \\
&= \tilde{Q}^{-1}\left[\vect{(X^\intercal UYV)} + Q_0 \mu_0 \right]
\end{align}
$$

Finally, if $U$ is the identity matrix, we get a further efficiency by precomputing $X^\intercal Y$

$$\tilde{\mu} = \tilde{Q}^{-1}\left[\vect{(X^\intercal YV)} + Q_0 \mu_0 \right]$$



## Normal Regression: beta

In this scenario, we look for the posterior for the model

$$y \sim \Norm(X\beta, Q)$$
$$\beta \sim \Norm(\mu_0, Q_0)$$

This is a specific case of the matrix normal regression, with $V \equiv 1$ and $U \equiv Q$.
We immediately get the posterior precision and mean:
$$\tilde{Q} = (X^\intercal Q X) + Q_0$$
$$\tilde{\mu} = \tilde{Q}^{-1}(X^\intercal Q y + Q_0 \mu_0)$$



We set $Q \equiv \tau I_p$ for independence among observations:

$$y \sim \Norm(X\beta, \tau I_p)$$
$$\beta \sim \Norm(\mu_0, Q_0)$$

We immediately get the posterior precision and mean:
$$\tilde{Q} = \tau X^\intercal X + Q_0$$
$$\tilde{\mu} = \tilde{Q}^{-1}(\tau X^\intercal y + Q_0 \mu_0)$$


## Matrix Normal: beta

In this scenario, we look for the posterior for the model

$$Y \sim \mathcal{MN}_{n,p}(\beta, U, V)$$
$$\vect(\beta) \sim \Norm(\mu_0, Q_0)$$

If there is only one realization, this is a specific case of the matrix normal regression, with $X \equiv I_n$.
We immediately get the posterior precision and mean:
$$\tilde{Q} = V \otimes U + Q_0$$
$$\tilde{\mu} = \tilde{Q}^{-1}\left[\vect{(UYV)} + Q_0 \mu_0 \right]$$

Again, if $U$ is the identity, we collapse even further to
$$\tilde{\mu} = \tilde{Q}^{-1}\left[\vect{(YV)} + Q_0 \mu_0 \right]$$


## Multivariate Normal: mu

In this scenario, we look for the posterior for the model

$$y \sim \Norm(\mu, Q)$$
$$\mu \sim \Norm(\mu_0, Q_0)$$

If there is only one realization, this is a specific case of the normal regression, with $X \equiv I_p$. However, usually more
than one realization is observed. We'll call the number of realizations $n$.

$$\tilde{Q} = nQ + Q_0$$
$$\tilde{\mu} = \tilde{Q}^{-1}(Q \sum_{i=1}^{n}y_i + Q_0 \mu_0)$$

## Normal: mu

In this scenario, we look for the posterior for the model

$$y \sim \Norm(\mu, \tau)$$
$$\mu \sim \Norm(\mu_0, \tau_0)$$

This is a specific case of the multivariate normal, with $Q \equiv \tau$.
We immediately get the posterior precision and mean:
$$\tilde{\tau} = n\tau + \tau_0$$
$$\tilde{\mu} = \tilde{\tau}^{-1}(\tau \sum_{i=1}^{n}y_i + \tau_0 \mu_0)$$


# Precisions

## Matrix Normal

In this scenario, we look for the posterior for the situation

$$Y \sim \mathcal{MN}_{n,p}(\beta, U, V)$$
$$V \sim \Wish(V_0, v_0)$$

We use the relevant parts of the convenient form of the matrix normal involving the trace, and multiply by the relevant parts of the Wishart density:

$$
\vert V \vert^{n/2}
\exp{(-\frac{1}{2}\Tr{[V(X - M)^\intercal U (X - M)]})} \cdot
\vert V \vert^{(v_0 - p - 1)/2}
\exp{(-\frac{1}{2}\Tr{[VV_0^{-1}]})}
$$

It's clear that the trace term becomes

$$\Tr{[V(V_0^{-1} + (X - M)^\intercal U (X - M))]}$$
and the determinant term becomes
$$\vert V \vert ^ {(n + v_0 - p - 1)/2}$$

So that we get
$$V \sim \Wish{((V_0^{-1} + (X - M)^\intercal U (X - M))^{-1}, n + v_0)}$$





