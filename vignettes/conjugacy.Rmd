---
title: "Conjugate distributions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Conjugate distributions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Below we derive the posterior distributions for various scenarios, given their conjugate priors.

# Means

## Matrix Normal Regression: beta


$\DeclareMathOperator{\vect}{vec}$
$\newcommand\Norm{\mathcal{N}}$

In this scenario, we look for the posterior for the model

$$y \sim \mathcal{MN}_{n,p}(X\beta, U, V)$$
with $X\beta$ an $n$ by $p$ matrix, U a $n$ by $n$ matrix, and V a $p$ by $p$ matrix.


We rewrite the model as
$$\vect(y) \sim \Norm_{np}(\vect(X\beta), V \otimes U)$$
with
$$\vect(\beta) \sim \Norm_(\mu_0, Q_0)$$

Let's consider the likelihood

$$
L(\beta\ \big|\ U, V, y, X, \mu_0, Q_0) \propto \exp\left\{-\frac{1}{2}(\vect(y) - \vect(X\beta))^\intercal (V \otimes U) (\vect(y) - \vect(X\beta)) +
  -\frac{1}{2}(\vect(\beta) - \mu_0)^\intercal Q_0 (\vect(\beta) - \mu_0)\right\}
$$

Expanding the inner part (ignoring the $-1/2$):

$$
\vect(y)^\intercal (V \otimes U) \vect(y)\\
- 2\vect(y)^\intercal(V \otimes U)\vect(X\beta)\\
+ \vect(X\beta)^\intercal (V \otimes U) \vect(X\beta)\\
+ \vect{\beta}^\intercal Q_0 \vect{\beta}\\
- 2\mu_o^\intercal Q_0 \beta\\
+ \mu_0^\intercal Q_0 \mu_0
$$

Note that $\vect(X\beta) = (I_p \otimes X)\vect(\beta)$. Then
$$
\vect(X\beta)^\intercal = \vect(\beta)^\intercal(I_p \otimes X)^\intercal = \vect(\beta)^\intercal (I_p \otimes X^\intercal)
$$

Dropping unused terms and regrouping:

$$
\vect(\beta)^\intercal ((I_p \otimes X^\intercal) (V \otimes U) (I_p \otimes X) + Q_0)\vect(\beta)\\
- 2(\vect(y)^\intercal(V \otimes U)(I_p \otimes X) + \mu_o^\intercal Q_0)\vect(\beta)
$$

The first line gives the posterior precision:

$$
\begin{align}
\tilde{Q} &= (I_p \otimes X^\intercal) (V \otimes U) (I_p \otimes X) + Q_0\\
&=(I_p \otimes X^\intercal) ((VI_p) \otimes (UX)) + Q_0\\
&= (I_pVI_p) \otimes (X^\intercal U X) + Q_0\\
&= V \otimes (X^\intercal U X) + Q_0
\end{align}
$$

The inverse of $\tilde{Q}$ is used to complete the square, and together with the transpose of the second line gives us the posterior mean:

$$
\begin{align}
\tilde{\mu} &= \tilde{Q}^{-1}((I_p \otimes X^\intercal) (V \otimes U) \vect{y} + Q_0 \mu_0) \\
&= \tilde{Q}^{-1}\left[((I_pV) \otimes (X^\intercal U))\vect{y} + Q_0 \mu_0 \right] \\
&= \tilde{Q}^{-1}\left[(V \otimes (X^\intercal U))\vect{y} + Q_0 \mu_0 \right]
\end{align}
$$


## Normal Regression: beta

In this scenario, we look for the posterior for the model

$$y \sim \mathcal{N}_{p}(X\beta, Q)$$
$$\beta \sim \Norm_(\mu_0, Q_0)$$

This is a specific case of the matrix normal regression, with $V \equiv 1$ and $U \equiv Q$.
We immediately get the posterior precision and mean:
$$\tilde{Q} = (X^\intercal Q X) + Q_0$$
$$\tilde{\mu} = \tilde{Q}^{-1}(X^\intercal Q y + Q_0 \mu_0)$$



Usually we set $Q = \equiv \tau I_p$:

$$y \sim \mathcal{N}_{p}(X\beta, \tau I_p)$$
$$\beta \sim \Norm_(\mu_0, Q_0)$$

We immediately get the posterior precision and mean:
$$\tilde{Q} = \tau X^\intercal X + Q_0$$
$$\tilde{\mu} = \tilde{Q}^{-1}(\tau X^\intercal y + Q_0 \mu_0)$$


## Matrix Normal: beta

In this scenario, we look for the posterior for the model

$$y \sim \mathcal{MN}_{n,p}(\beta, U, V)$$
$$\vect(\beta) \sim \Norm_(\mu_0, Q_0)$$

If there is only one realization, this is a specific case of the matrix normal regression, with $X \equiv I_n$.
We immediately get the posterior precision and mean:
$$\tilde{Q} = V \otimes U + Q_0$$
$$\tilde{\mu} = \tilde{Q}^{-1}\left[(V \otimes U)\vect{y} + Q_0 \mu_0 \right]$$

## Multivariate Normal: mu

In this scenario, we look for the posterior for the model

$$y \sim \mathcal{N}_{p}(\mu, Q)$$
$$\mu \sim \Norm_(\mu_0, Q_0)$$

If there is only one realization, this is a specific case of the normal regression, with $X \equiv I_p$. However, usually more
than one realization is observed. We'll call the number of realizations $n$.

This is a specific case of the normal regression, with $X \equiv I_n$.
We immediately get the posterior precision and mean:
$$\tilde{Q} = nQ + Q_0$$
$$\tilde{\mu} = \tilde{Q}^{-1}(Q \sum_{i=1}^{n}y_i + Q_0 \mu_0)$$

## Normal: mu

In this scenario, we look for the posterior for the model

$$y \sim \mathcal{N}(\mu, \tau)$$
$$\mu \sim \Norm_(\mu_0, \tau_0)$$

This is a specific case of the multivariate normal, with $Q \equiv \tau$.
We immediately get the posterior precision and mean:
$$\tilde{\tau} = n\tau + \tau_0$$
$$\tilde{\mu} = \tilde{\tau}^{-1}(\tau \sum_{i=1}^{n}y_i + \tau_0 \mu_0)$$

